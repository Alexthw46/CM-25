{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "from benchmark_utils import *"
   ],
   "id": "24829f1793eada1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n = 100\n",
    "m = 100\n",
    "density = 0.1  # Probability of observing each entry\n",
    "\n",
    "# seed\n",
    "seed = int(time.time())\n",
    "\n",
    "# Generate synthetic problem\n",
    "X_true, X_obs, mask, u_true, v_true = generate_synthetic_problem(m, n, density,\n",
    "                                                                 seed - 1)  # seed-1 to avoid overlap with the gaussian initialization seed as they'd be the same otherwise"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Baseline using Truncated SVD with imputed values\n",
    "print(\"\\n=== Truncated SVD ===\")\n",
    "observed_error_svd, full_error_svd, time_svd = baseline_svd_numpy(X_true, X_obs, mask)\n",
    "print(f\"SVD: Observed Error={observed_error_svd:.6f}, Full Error={full_error_svd:.6f}, Time={time_svd:.4f}s\")"
   ],
   "id": "e2d4e2feca64b1ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Benchmark the different initialization strategies\n",
    "print(\"=== Gaussian Initialization ===\")\n",
    "u0_g, v0_g = initialize_uv(X_obs, mask, strategy='gaussian', seed=seed)\n",
    "# Benchmark the different solvers\n",
    "gauss = compare_solvers(X_obs, X_true, u0_g.copy(), v0_g.copy(), mask, plot=True, gd_params=[1.78e-3, 7.94e-01, 0.75],\n",
    "                        max_it=300 * 1000, patience=1000)"
   ],
   "id": "5e0324d5a56ec9af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n=== SVD Initialization ===\")\n",
    "u0_s, v0_s = initialize_uv(X_obs, mask, strategy='svd', seed=seed)\n",
    "svd = compare_solvers(X_obs, X_true, u0_s.copy(), v0_s.copy(), mask, lambda_als=[1e-8, 1e-10],\n",
    "                      gd_params=[1e-02, 1e-08, 0.75],\n",
    "                      plot=True, max_it=300 * 1000, patience=1000)"
   ],
   "id": "6ac7629659caa6e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n=== SVD + Noise Initialization ===\")\n",
    "u0_sn, v0_sn = initialize_uv(X_obs, mask, strategy='svd', epsilon=0.1, seed=seed)\n",
    "svd_n = compare_solvers(X_obs, X_true, u0_sn.copy(), v0_sn.copy(), mask, lambda_als=[3e-8, 1e-10],\n",
    "                        gd_params=[1.78e-02, 1e-8, 0.75], plot=True, max_it=300 * 1000, patience=1000)"
   ],
   "id": "d1fe8aeafca368d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\n=== Mean Initialization ===\")\n",
    "u0_m, v0_m = initialize_uv(X_obs, mask, strategy='mean', seed=seed)\n",
    "mean = compare_solvers(X_obs, X_true, u0_m.copy(), v0_m.copy(), mask, lambda_als=[1e-0, 5.80e-03],\n",
    "                       gd_params=[7.74e-05, 5.62e-01, 0.9], plot=True, max_it=1000, patience=1000)"
   ],
   "id": "d35c819ca879ebd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# compare the results of the different initializations and solvers in a table\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_ALS = pd.DataFrame({\n",
    "    'Method': ['Gaussian', 'SVD', 'SVD + Noise', 'Mean'],\n",
    "    'Observed Error': [\n",
    "        gauss['ALS']['observed_error'],\n",
    "        svd['ALS']['observed_error'],\n",
    "        svd_n['ALS']['observed_error'],\n",
    "        mean['ALS']['observed_error']\n",
    "    ],\n",
    "    'Full Error': [\n",
    "        gauss['ALS']['full_error'],\n",
    "        svd['ALS']['full_error'],\n",
    "        svd_n['ALS']['full_error'],\n",
    "        mean['ALS']['full_error']\n",
    "    ],\n",
    "    'Time': [\n",
    "        gauss['ALS']['time'],\n",
    "        svd['ALS']['time'],\n",
    "        svd_n['ALS']['time'],\n",
    "        mean['ALS']['time']\n",
    "    ]\n",
    "})\n",
    "# Convert the DataFrame to a string table\n",
    "table_str = tabulate(results_ALS, headers='keys', tablefmt='pretty', showindex=False)\n",
    "# Print the table\n",
    "print(\"=== Alternating Optimization (ALS) ===\")\n",
    "print(table_str)\n",
    "\n",
    "results_NormALS = pd.DataFrame({\n",
    "    'Method': ['Gaussian', 'SVD', 'SVD + Noise', 'Mean'],\n",
    "    'Observed Error': [\n",
    "        gauss['NormALS']['observed_error'],\n",
    "        svd['NormALS']['observed_error'],\n",
    "        svd_n['NormALS']['observed_error'],\n",
    "        mean['NormALS']['observed_error']\n",
    "    ],\n",
    "    'Full Error': [\n",
    "        gauss['NormALS']['full_error'],\n",
    "        svd['NormALS']['full_error'],\n",
    "        svd_n['NormALS']['full_error'],\n",
    "        mean['NormALS']['full_error']\n",
    "    ],\n",
    "    'Time': [\n",
    "        gauss['NormALS']['time'],\n",
    "        svd['NormALS']['time'],\n",
    "        svd_n['NormALS']['time'],\n",
    "        mean['NormALS']['time']\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Convert the DataFrame to a string table\n",
    "table_str = tabulate(results_NormALS, headers='keys', tablefmt='pretty', showindex=False)\n",
    "# Print the table\n",
    "print(\"=== Alternating Optimization with Normalization (NormALS) ===\")\n",
    "print(table_str)\n",
    "\n",
    "results_gd = pd.DataFrame({\n",
    "    'Method': ['Gaussian', 'SVD', 'SVD + Noise', 'Mean'],\n",
    "    'Observed Error': [\n",
    "        gauss['GD']['observed_error'],\n",
    "        svd['GD']['observed_error'],\n",
    "        svd_n['GD']['observed_error'],\n",
    "        mean['GD']['observed_error']\n",
    "    ],\n",
    "    'Full Error': [\n",
    "        gauss['GD']['full_error'],\n",
    "        svd['GD']['full_error'],\n",
    "        svd_n['GD']['full_error'],\n",
    "        mean['GD']['full_error']\n",
    "    ],\n",
    "    'Time': [\n",
    "        gauss['GD']['time'],\n",
    "        svd['GD']['time'],\n",
    "        svd_n['GD']['time'],\n",
    "        mean['GD']['time']\n",
    "    ]\n",
    "})\n",
    "# Convert the DataFrame to a string table\n",
    "table_str = tabulate(results_gd, headers='keys', tablefmt='pretty', showindex=False)\n",
    "# Print the table\n",
    "print(\"=== Gradient Descent (GD) ===\")\n",
    "print(table_str)\n"
   ],
   "id": "f8209f8692446650",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Benchmarking over 50 seeds",
   "id": "1e2b906ada832b9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "init_settings = {\n",
    "    'gaussian': {\n",
    "        'lambda_als': [0.403, 1.5e-2],\n",
    "        'gd_params': [1.78e-03, 7.94e-01, 0.75]\n",
    "    },\n",
    "    'svd': {\n",
    "        'lambda_als': [1e-8, 1e-10],\n",
    "        'gd_params': [1.78e-02, 1e-08, 0.75]\n",
    "    },\n",
    "    'svd+noise': {\n",
    "        'lambda_als': [3e-8, 1e-10],\n",
    "        'gd_params': [1e-02, 1e-8, 0.75]\n",
    "    },\n",
    "    'mean': {\n",
    "        'lambda_als': [1e-0, 5.80e-03],\n",
    "        'gd_params': [7.74e-05, 5.62e-01, 0.9]\n",
    "    }\n",
    "}"
   ],
   "id": "5f82eb9c51676019",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Time the initialization strategies\n",
    "for strategy in ['gaussian', 'svd', 'mean']:\n",
    "    init_times = []\n",
    "    for seed in range(50):\n",
    "        start = time.time()\n",
    "        u0, v0 = initialize_uv(X_obs, mask, strategy=strategy, seed=seed)\n",
    "        end = time.time()\n",
    "        init_times.append(end - start)\n",
    "    print(f\"Initialization strategy: {strategy}, Average time over 50 seeds: {np.mean(init_times):.6f}s\")\n",
    "\n"
   ],
   "id": "f641221542eb1b87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Baseline using Truncated SVD",
   "id": "2e651f496b0099e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# baseline using Truncated SVD with imputed values over 50 seeds\n",
    "svd_observed_errors = []\n",
    "svd_full_errors = []\n",
    "for seed in range(50):\n",
    "    X_true, X_obs, mask, u_true, v_true = generate_synthetic_problem(m, n, density,\n",
    "                                                                     seed)\n",
    "    observed_error_svd, full_error_svd, time_svd = baseline_svd(X_true, X_obs, mask)\n",
    "    svd_observed_errors.append(observed_error_svd)\n",
    "    svd_full_errors.append(full_error_svd)\n",
    "\n",
    "# Convert the results to a DataFrame for better visualization\n",
    "svd_results = pd.DataFrame({\n",
    "    'Observed Error': svd_observed_errors,\n",
    "    'Full Error': svd_full_errors,\n",
    "})\n",
    "print(\"=== Truncated SVD Results (mean over 20 seeds) ===\")\n",
    "print(tabulate(svd_results.mean().reset_index(), headers=['Metric', 'Value'], tablefmt='pretty', showindex=False))\n"
   ],
   "id": "59bdd4f085d983ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Benchmarking over 50 seeds at 0.1 density",
   "id": "e5c17449e7781ef5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Structure: method -> solver -> list of errors\n",
    "accum_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for seed in range(50):\n",
    "    seed_results = run_benchmark_for_seed(seed + 1, density=0.1, m=m, n=n, init_settings=init_settings)\n",
    "    for method, solvers in seed_results.items():\n",
    "        for solver, vals in solvers.items():\n",
    "            accum_results[method][f'{solver}_obs'].append(vals['observed_error'])\n",
    "            accum_results[method][f'{solver}_full'].append(vals['full_error'])\n",
    "            accum_results[method][f'{solver}_time'].append(vals['time'])\n",
    "            accum_results[method][f'{solver}_iterations'].append(vals['iterations'])\n",
    "\n",
    "# === PRINT TABLES ===\n",
    "for solver in ['ALS', 'NormALS', 'GD', 'Baseline SVD']:\n",
    "    print(f\"=== {solver} ===\")\n",
    "    df = summarize_solver_results(solver, accum_results)\n",
    "    print(tabulate(df, headers='keys', tablefmt='pretty', showindex=False))\n",
    "    print()"
   ],
   "id": "f9b30f2442e8f862",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Benchmarking over 50 seeds at different densities",
   "id": "cfd961864e6c1a93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Densities and categories\n",
    "densities = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "solvers = ['ALS', 'NormALS', 'GD', 'Baseline SVD']  # Baseline SVD is included for completeness\n",
    "methods = ['Gaussian', 'Svd', 'Mean']\n",
    "metrics = {\n",
    "    'Observed Error': 'obs',\n",
    "    'Full Error': 'full',\n",
    "    'Time': 'time',\n",
    "    'Iterations': 'iterations',\n",
    "}\n",
    "# Defines distinct markers for plotting\n",
    "method_markers = {\n",
    "    'Gaussian': 'o',\n",
    "    'Svd': 's',\n",
    "    'Mean': 'D',\n",
    "    'noise': '^',\n",
    "}"
   ],
   "id": "affbfbb9cd21a77d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "accum_results = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "\n",
    "for density in densities:\n",
    "    print(f\"Running benchmark for density={density}\")\n",
    "    for seed in range(50):\n",
    "        seed_results = run_benchmark_for_seed(seed + 1, density=density, m=m, n=n, init_settings=init_settings)\n",
    "\n",
    "        for method, solvers in seed_results.items():\n",
    "            for solver, vals in solvers.items():\n",
    "                accum_results[density][method][f'{solver}_obs'].append(vals['observed_error'])\n",
    "                accum_results[density][method][f'{solver}_full'].append(vals['full_error'])\n",
    "                accum_results[density][method][f'{solver}_time'].append(vals['time'])\n",
    "                accum_results[density][method][f'{solver}_iterations'].append(vals['iterations'])\n",
    "\n",
    "    # === PRINT TABLES ===\n",
    "    for solver in ['ALS', 'NormALS', 'GD', 'Baseline SVD']:\n",
    "        print(f\"=== {solver} ===\")\n",
    "        df = summarize_solver_results(solver, accum_results[density])\n",
    "        print(tabulate(df, headers='keys', tablefmt='pretty', showindex=False))\n",
    "        print()"
   ],
   "id": "93d5e3b3fca7f9c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Loop over metrics and generate plots\n",
    "for metric_name, suffix in metrics.items():\n",
    "    # Collect metric values: key = \"Solver + Method\"\n",
    "    metric_data = defaultdict(list)\n",
    "\n",
    "    for density in densities:\n",
    "        for solver in solvers:\n",
    "            if solver == 'Baseline SVD':\n",
    "                # Special case for Baseline SVD\n",
    "                key = f\"{solver}\"\n",
    "                try:\n",
    "                    value = np.mean(accum_results[density][solver][f\"{solver}_{suffix}\"])\n",
    "                    metric_data[key].append(value)\n",
    "                except KeyError:\n",
    "                    pass  # in case of missing data\n",
    "            else:\n",
    "                for method in methods:\n",
    "                    key = f\"{solver} + {method}\"\n",
    "                    try:\n",
    "                        value = np.mean(accum_results[density][method][f\"{solver}_{suffix}\"])\n",
    "                        metric_data[key].append(value)\n",
    "                    except KeyError:\n",
    "                        pass  # in case of missing data\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for label, values in metric_data.items():\n",
    "        method = label.split(' + ')[-1]\n",
    "        marker = method_markers.get(method, 'x')  # Default marker\n",
    "        plt.plot(densities, values, marker=marker, label=label)\n",
    "\n",
    "    plt.xlabel('Density')\n",
    "    plt.ylabel(metric_name + (' (log scale)' if 'Error' in metric_name else ''))\n",
    "    plt.title(f'{metric_name} vs Density')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.xticks(densities)\n",
    "    plt.yscale('log')\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{suffix}_vs_density.pdf')\n",
    "    plt.show()"
   ],
   "id": "9c0446f62f4d407f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot without the svd to not alter the y-scale\n",
    "methods = ['Gaussian', 'Mean']\n",
    "\n",
    "# Loop over metrics and generate plots\n",
    "for metric_name, suffix in metrics.items():\n",
    "    # Collect metric values: key = \"Solver + Method\"\n",
    "    metric_data = defaultdict(list)\n",
    "\n",
    "    for density in densities:\n",
    "        for solver in solvers:\n",
    "            if solver == 'Baseline SVD':\n",
    "                # Special case for Baseline SVD\n",
    "                key = f\"{solver}\"\n",
    "                try:\n",
    "                    value = np.mean(accum_results[density][solver][f\"{solver}_{suffix}\"])\n",
    "                    metric_data[key].append(value)\n",
    "                except KeyError:\n",
    "                    pass  # in case of missing data\n",
    "            else:\n",
    "                for method in methods:\n",
    "                    key = f\"{solver} + {method}\"\n",
    "                    try:\n",
    "                        value = np.mean(accum_results[density][method][f\"{solver}_{suffix}\"])\n",
    "                        metric_data[key].append(value)\n",
    "                    except KeyError:\n",
    "                        pass  # in case of missing data\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for label, values in metric_data.items():\n",
    "        method = label.split(' + ')[-1]\n",
    "        marker = method_markers.get(method, 'x')  # Default marker\n",
    "        plt.plot(densities, values, marker=marker, label=label)\n",
    "\n",
    "    plt.xlabel('Density')\n",
    "    plt.ylabel(metric_name + (' (log scale)' if 'Error' in metric_name else ''))\n",
    "    plt.title(f'{metric_name} vs Density')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.xticks(densities)\n",
    "    plt.yscale('log')\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{suffix}_vs_density_z.pdf')\n",
    "    plt.show()"
   ],
   "id": "2598d43e35171013",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Benchmarking over 50 seeds with noised data",
   "id": "79d39f4fb0b73ddc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "snrs = [40, 20, 10, 5, 2]",
   "id": "ab967b7862e4fb6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Accumulate results ===\n",
    "accum_results = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "\n",
    "for snr in snrs:\n",
    "    print(f\"Running benchmark for SNR={snr}\")\n",
    "    for seed in range(50):\n",
    "        seed_results = run_benchmark_for_seed(\n",
    "            seed + 1,\n",
    "            density=0.1,\n",
    "            m=m, n=n,\n",
    "            init_settings=init_settings,\n",
    "            snr=snr\n",
    "        )\n",
    "\n",
    "        for method, solvers_dict in seed_results.items():\n",
    "            for solver, vals in solvers_dict.items():\n",
    "                accum_results[snr][method][f'{solver}_obs'].append(vals['observed_error'])\n",
    "                accum_results[snr][method][f'{solver}_full'].append(vals['full_error'])\n",
    "                accum_results[snr][method][f'{solver}_time'].append(vals['time'])\n",
    "                accum_results[snr][method][f'{solver}_iterations'].append(vals['iterations'])\n",
    "\n",
    "    for solver in solvers:\n",
    "        print(f\"=== {solver} ===\")\n",
    "        df = summarize_solver_results(solver, accum_results[snr])\n",
    "        print(tabulate(df, headers='keys', tablefmt='pretty', showindex=False))\n",
    "        print()\n"
   ],
   "id": "86b558a2880c7112",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Plot each metric vs SNR ===\n",
    "for metric_name, suffix in metrics.items():\n",
    "    metric_data = defaultdict(list)\n",
    "\n",
    "    for snr in snrs:\n",
    "        for solver in solvers:\n",
    "            if solver == 'Baseline SVD':\n",
    "                key = f\"{solver}\"\n",
    "                try:\n",
    "                    value = np.mean(accum_results[snr][solver][f\"{solver}_{suffix}\"])\n",
    "                    metric_data[key].append(value)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            else:\n",
    "                for method in methods:\n",
    "                    key = f\"{solver} + {method}\"\n",
    "                    try:\n",
    "                        value = np.mean(accum_results[snr][method][f\"{solver}_{suffix}\"])\n",
    "                        metric_data[key].append(value)\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "\n",
    "    # === Plot ===\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if 'Error' in metric_name:\n",
    "        plt.yscale('log')\n",
    "        metric_name = 'Relative ' + metric_name\n",
    "    for label, values in metric_data.items():\n",
    "        method = label.split(' + ')[-1]\n",
    "        marker = method_markers.get(method, 'x')\n",
    "        plt.plot(snrs, values, marker=marker, label=label)\n",
    "\n",
    "    plt.xlabel('SNR')\n",
    "    plt.ylabel(metric_name + (' (log scale)' if 'Error' in metric_name else ''))\n",
    "    plt.title(f'{metric_name} vs SNR')\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{suffix}_vs_snr.pdf')\n",
    "    plt.show()\n"
   ],
   "id": "2c4a1bd3d65a7d12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cc34a7d759040963",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
