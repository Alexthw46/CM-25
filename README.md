# 1. Introduction

## 1.1 Observed Data

Let M âˆˆ â„^{m Ã— n} be an unknown matrix. Instead of the full matrix, we are given a subset of entries:

 
 
ğ’Ÿ = { (iâ‚–, jâ‚–, xâ‚–) âˆˆ {1,â€¦,m} Ã— {1,â€¦,n} Ã— â„ }â‚–=1,â€¦,p where xâ‚– = M_{iâ‚– jâ‚–}.
Our goal is to approximate M with a rank-1 matrix using this incomplete information.

## 1.2 Rank-1 Matrix Approximation

We seek vectors u âˆˆ â„^m and v âˆˆ â„^n such that A = u váµ€
minimizes the squared error:
min_{u,v} âˆ‘â‚– (u_{iâ‚–} v_{jâ‚–} - xâ‚–)Â²

This reduces to the classic Eckartâ€“Youngâ€“Mirsky theorem if all entries are available.

## 1.3 Matrix Representation and Sampling Operator

We define a sampling operator ğ’«_Î© that extracts the observed entries:
ğ’«_Î©(A) = { A_{iâ‚– jâ‚–} }â‚–=1,â€¦,p

Let Î© = { (iâ‚–, jâ‚–) } be the index set of observations.

The objective becomes: min_{u,v} ||ğ’«_Î©(u váµ€) - x||Â²â‚‚

## 1.4 Scale Ambiguity and Normalization

The decomposition A = u váµ€ is not unique due to scaling: for any nonzero Î±,
 
(Î±u)(váµ€ / Î±) = u váµ€

To eliminate this ambiguity, we may fix a normalization such as ||v|| = 1. This is always achievable by rescaling u.

Note: constraints like ||u||Â² + ||v||Â² = 1 are not always feasible (see the m = n = 1, u = 100, v = 1 case).

# 2. Mathematical Analysis

## 2.1 Objective Structure: Bi-convexity
The objective function f(u, v) = âˆ‘â‚– (u_{iâ‚–} v_{jâ‚–} - xâ‚–)Â²
is not jointly convex, but bi-convex:

* f(Â·, v) is convex in u for fixed v

* f(u, Â·) is convex in v for fixed u

This allows optimization using Alternating Least Squares (ALS).

## 2.2 Gradient Computation
Partial derivatives are:

* âˆ‚f/âˆ‚uáµ¢ = 2 âˆ‘_{k : iâ‚– = i} (uáµ¢ v_{jâ‚–} - xâ‚–)v_{jâ‚–}
* âˆ‚f/âˆ‚vâ±¼ = 2 âˆ‘_{k : jâ‚– = j} (u_{iâ‚–} vâ±¼ - xâ‚–)u_{iâ‚–}

Efficient to compute using only observed entries.

## 2.3 Hessian and Non-Convexity
The full Hessian of f(u, v) is not positive semidefinite, due to bilinear terms.

This implies the landscape may contain local minima and saddle points â€” global convergence is not guaranteed.

## 2.4 Solution Uniqueness, Normalization, and Regularization
Scale ambiguity is broken by imposing valid normalizations, such as ||v|| = 1. Improper constraints (e.g. ||u||Â² + ||v||Â² = 1) may not be satisfiable via rescaling.

If observations are sparse, multiple solutions can match the data.

As an alternative, we use â„“â‚‚ regularization to stabilize optimization:
f_Î»(u, v) = f(u, v) + Î» (||u||Â² + ||v||Â²)

This penalizes large magnitudes but doesn't enforce a specific norm.

Note: Regularization improves generalization but does not eliminate scale ambiguity alone.

## 2.5 Degrees of Freedom
We estimate m + n parameters from p observations. To avoid underdetermined systems:
 
p â‰« m + n
must hold for identifiability and stability.

# References
Eckart, C., & Young, G. (1936). "The approximation of one matrix by another of lower rank."

CandÃ¨s, E. J., & Recht, B. (2009). "Exact matrix completion via convex optimization."
 
